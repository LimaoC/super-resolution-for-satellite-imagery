{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4HaVuWd04qM"
   },
   "source": [
    "# Super Resolution Diffusion Model Training Code\n",
    "## Group 6 Super Resolution Project\n",
    "\n",
    "Written following the guide at:\n",
    "\n",
    "https://huggingface.co/docs/diffusers/en/tutorials/basic_training\n",
    "\n",
    "and with reference to\n",
    "\n",
    "https://arxiv.org/pdf/2104.07636\n",
    "\n",
    "https://arxiv.org/pdf/2006.11239"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StLbGiquzrzI"
   },
   "source": [
    "## Training Code\n",
    "\n",
    "### Imports\n",
    "Make sure to install our package beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C1VKJwEmys3K"
   },
   "outputs": [],
   "source": [
    "\n",
    "from super_resolution.src.sen2venus_dataset import S2VSite, S2VSites, create_train_test_split\n",
    "from super_resolution.src.visualization import plot_gallery\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import interpolate\n",
    "import py7zr as py7zr\n",
    "\n",
    "import diffusers\n",
    "import accelerate\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from diffusers import DDPMScheduler\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from diffusers import DDPMPipeline\n",
    "from diffusers import UNet2DModel\n",
    "\n",
    "\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from accelerate import notebook_launcher\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQf2JX-cz3N-"
   },
   "source": [
    "### Defining Training Configuration\n",
    "\n",
    "All the training parameters are set here for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V08tfnPN0IFC"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Training Configuration\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 256  # the generated image resolution\n",
    "    train_batch_size = 16 # how many images to sample during training\n",
    "    num_epochs = 25\n",
    "    train_sites = {\"SO2\", \"FR-BIL\", \"NARYN\"}\n",
    "    data_dir = \"/content/drive/MyDrive/STAT3007 Project Data/\"\n",
    "    output_dir = \"/content/drive/MyDrive/STAT3007'\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_image_epochs = 10\n",
    "    save_model_epochs = 1\n",
    "    mixed_precision = 'no'  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
    "    seed = 0\n",
    "\n",
    "config = TrainingConfig()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGu0RyrWz-nQ"
   },
   "source": [
    "### Loading Data\n",
    "\n",
    "Just on SO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGICmhtv1brA"
   },
   "outputs": [],
   "source": [
    "def clamp_transform(x, y):\n",
    "    x = x[:3, :, :]\n",
    "    y = y[:3, :, :]\n",
    "\n",
    "    x = torch.clamp(x, 0, 1)\n",
    "    y = torch.clamp(y, 0, 1)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "train_data, test_data = create_train_test_split(\n",
    "    data_dir = config.data_dir,\n",
    "    seed = -1,\n",
    "    sites = config.train_sites,\n",
    "    device = config.device,\n",
    ")\n",
    "\n",
    "train_data.set_transform(clamp_transform)\n",
    "test_data.set_transform(clamp_transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=config.train_batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwcmeXdb0Jg0"
   },
   "source": [
    "### Defining Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q5J0zrhn0Nd3"
   },
   "outputs": [],
   "source": [
    "def normalize_tensor(tensor):\n",
    "    min_val = tensor.min()\n",
    "    max_val = tensor.max()\n",
    "    normalized_tensor = (tensor - min_val) / (max_val - min_val)\n",
    "    return normalized_tensor\n",
    "# # preprocessing\n",
    "process = nn.Sequential(\n",
    "    transforms.Lambda(normalize_tensor)\n",
    ")\n",
    "\n",
    "upscale = lambda x: interpolate(x, size=(256, 256), mode=\"bicubic\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTtBietJ0Q5n"
   },
   "source": [
    "### Defining the U-Net to be used\n",
    "\n",
    "Some fine tuning can be done here by modifying the underlying U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JIcDqgCM26oj"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = UNet2DModel(\n",
    "    sample_size=config.image_size,  # the target image resolution\n",
    "    in_channels=6,  # 6 channels since the input is a concat of noise + upscaled low res\n",
    "    out_channels=3,  # 3 RGB out channels\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "     # the number of output channels for each UNet block\n",
    "    block_out_channels=(128, 128, 128, 256, 256, 256),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\"\n",
    "      ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Co3uVX0R0fMM"
   },
   "source": [
    "### Noise Pipelines and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KO33hPTf3B6K"
   },
   "outputs": [],
   "source": [
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000) # forward process scheduler\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = config.learning_rate)\n",
    "\n",
    "lr_scheduler = get_cosine_schedule_with_warmup( \n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(len(train_dataloader) * config.num_epochs)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vQewu-10n6W"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQtyu4xk3XUF"
   },
   "outputs": [],
   "source": [
    "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler, upscaler):\n",
    "    # Initialize accelerator and tensorboard logging\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        log_with=\"tensorboard\",\n",
    "        project_dir=os.path.join(config.output_dir, \"logs\"),\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        if config.output_dir is not None:\n",
    "            os.makedirs(config.output_dir, exist_ok=True)\n",
    "        accelerator.init_trackers(\"train_example\")\n",
    "\n",
    "    # Prepare the model and optimizer\n",
    "    model, optimizer, train_dataloader, lr_scheduler, upscaler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, lr_scheduler, upscaler\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            low, high = batch \n",
    "            upscaled = upscaler(low) # naive upscale \n",
    "\n",
    "            # Sample noise to add to the images\n",
    "            noise = torch.randn(high.shape, device=high.device)\n",
    "            bs = high.shape[0]\n",
    "\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(\n",
    "                0, noise_scheduler.config.num_train_timesteps, (bs,), device=high.device,\n",
    "                dtype=torch.int64\n",
    "            )\n",
    "\n",
    "            # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process)\n",
    "            noisy_images = noise_scheduler.add_noise(high, noise, timesteps)\n",
    "\n",
    "            with accelerator.accumulate(model):\n",
    "                # concatenate the upscaled image to the noise\n",
    "                noisy_images = torch.concat([noisy_images, upscaled], dim=1)\n",
    "                # predict the next latent \n",
    "                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "                \n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "            global_step += 1\n",
    "\n",
    "        \n",
    "        if accelerator.is_main_process:\n",
    "            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
    "            pipeline.save_pretrained(config.output_dir) # save results each epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LcmKXkCJ1uPt"
   },
   "source": [
    "### Launch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "og_ok9rq3Zji"
   },
   "outputs": [],
   "source": [
    "args = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler, upscale)\n",
    "\n",
    "notebook_launcher(train_loop, args, num_processes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiIDp85W18bM"
   },
   "source": [
    "## Testing Code\n",
    "\n",
    "The first few cells are carried over from the Training section so that this can be run independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StLbGiquzrzI"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from super_resolution.src.sen2venus_dataset import create_train_test_split\n",
    "from super_resolution.src.visualization import plot_gallery\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import interpolate\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torchvision.transforms.functional import adjust_brightness\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from diffusers import DDPMPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGu0RyrWz-nQ"
   },
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGICmhtv1brA"
   },
   "outputs": [],
   "source": [
    "def clamp_transform(x, y):\n",
    "    x = x[:3, :, :]\n",
    "    y = y[:3, :, :]\n",
    "\n",
    "    x = torch.clamp(x, 0, 1)\n",
    "    y = torch.clamp(y, 0, 1)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "train_data, test_data = create_train_test_split(\n",
    "    data_dir = \"../data/sites\",\n",
    "    seed = -1,\n",
    "    sites = {\"SO2\"}, #{\"SO2\", \"FR-BIL\", \"NARYN\"},\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "\n",
    "# We only need the test dataloader here\n",
    "test_data.set_transform(clamp_transform)\n",
    "test_dataloader = DataLoader(test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwcmeXdb0Jg0"
   },
   "source": [
    "### Defining Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q5J0zrhn0Nd3"
   },
   "outputs": [],
   "source": [
    "def normalize_tensor(tensor):\n",
    "    min_val = tensor.min()\n",
    "    max_val = tensor.max()\n",
    "    normalized_tensor = (tensor - min_val) / (max_val - min_val)\n",
    "    return normalized_tensor\n",
    "\n",
    "# preprocessing\n",
    "process = nn.Sequential(\n",
    "    transforms.Lambda(normalize_tensor)\n",
    ")\n",
    "\n",
    "upscale = lambda x: interpolate(x, size=(256, 256), mode=\"bicubic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNspQdqn2Drg"
   },
   "source": [
    "###  Load Model\n",
    "\n",
    "Will need to be configured to the location of the model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEH3WvBoCyGo"
   },
   "outputs": [],
   "source": [
    "model_location = \"../models/FinalModelEpoch12\"\n",
    "loadModel = DDPMPipeline.from_pretrained(model_location)\n",
    "loadModel.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zr6sddhk2PA3"
   },
   "source": [
    "### Testing Code Loop\n",
    "Generates a High resolution sample from a low resolution input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r07Yxv36eHg4"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def test_diffuse(pipeline, upscaler, low, generator = None, process = None):\n",
    "\n",
    "    batch_size = low.shape[0]\n",
    "    noise = torch.randn(\n",
    "            (batch_size, 3, pipeline.unet.sample_size, pipeline.unet.sample_size),\n",
    "            generator=generator,\n",
    "        )\n",
    "    noise = noise.to(pipeline.device)\n",
    "\n",
    "    upscaled = upscaler(low)\n",
    "    upscaled = upscaled.to(pipeline.device)\n",
    "    # set step values\n",
    "    pipeline.scheduler.set_timesteps(100)\n",
    "\n",
    "    for t in pipeline.progress_bar(pipeline.scheduler.timesteps):\n",
    "            # predict noise model_output after concatenating the upscaled\n",
    "            image = torch.concat([noise, upscaled], dim=1)\n",
    "            model_output = pipeline.unet(image, t).sample\n",
    "\n",
    "            # compute previous latent: x_t -> t_t-1\n",
    "            noise = pipeline.scheduler.step(model_output, t, noise, generator=generator).prev_sample\n",
    "\n",
    "    noise = noise.clamp(0,1)\n",
    "\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAb4Xla92Xal"
   },
   "source": [
    "### Generating Test Images\n",
    "\n",
    "To generate a single upscaled image use this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fu3WEU7AQ4Ox"
   },
   "outputs": [],
   "source": [
    "diffusionSR = lambda x: test_diffuse(loadModel, upscale, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNT-DIpW34Bl"
   },
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNx4bhLJI-vX"
   },
   "outputs": [],
   "source": [
    "NUM_IMAGES = 1\n",
    "\n",
    "test_imgs = []\n",
    "titles = []\n",
    "for (i, img) in enumerate(test_dataloader):\n",
    "    if i > 10*(NUM_IMAGES - 1):\n",
    "        break\n",
    "    if i % 10 == 0:\n",
    "        titles.append(f\"Low Res {i}\")\n",
    "        titles.append(f\"Naive Upscale {i}\")\n",
    "        titles.append(f\"Upscaled {i}\")\n",
    "        titles.append(f\"High Res {i}\")\n",
    "        low, high = img\n",
    "        # low, high = process(low), process(high)\n",
    "        upscaled = test_diffuse(loadModel, upscale, low, process = process)\n",
    "        test_imgs.append(low.squeeze(0).permute(2,1,0))\n",
    "        test_imgs.append(upscale(low).squeeze(0).permute(2,1,0))\n",
    "        test_imgs.append(upscaled.squeeze(0).permute(2,1,0))\n",
    "        test_imgs.append(high.squeeze(0).permute(2,1,0))\n",
    "\n",
    "test_imgs = [t.cpu() for t in test_imgs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgrUzE_Rcs55"
   },
   "outputs": [],
   "source": [
    "plot_gallery(test_imgs, titles, nrow = NUM_IMAGES, xscale = 3, yscale = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single_patch(idx, brightness=None):\n",
    "    i, img = next((i, img) for (i, img) in enumerate(test_dataloader) if i == idx)\n",
    "    low, high = img\n",
    "    upscaled = test_diffuse(loadModel, upscale, low, process=process)\n",
    "    imgs = [low, upscale(low), upscaled, high]\n",
    "    if brightness:\n",
    "        imgs = [adjust_brightness(img, brightness).squeeze(0).permute(1,2,0).cpu() for img in imgs]\n",
    "    else:\n",
    "        imgs = [img.squeeze(0).permute(1,2,0).cpu() for img in imgs]\n",
    "    titles = [\"Low Res\", \"Naive Upscale\", \"Upscaled\", \"High Res\"]\n",
    "\n",
    "    plot_gallery(imgs, titles, nrow=1, xscale=3, yscale=3)\n",
    "\n",
    "def save_single_patch(idx, brightness=None):\n",
    "    i, img = next((i, img) for (i, img) in enumerate(test_dataloader) if i == idx)\n",
    "    low, high = img\n",
    "    upscaled = test_diffuse(loadModel, upscale, low, process=process)\n",
    "    imgs = [low, upscale(low), upscaled, high]\n",
    "    if brightness:\n",
    "        imgs = [adjust_brightness(img, brightness).squeeze(0).cpu() for img in imgs]\n",
    "    else:\n",
    "        imgs = [img.squeeze(0).cpu() for img in imgs]\n",
    "    save_filenames = [\"low_res\", \"naive_upscale\", \"upscaled\", \"high_res\"]\n",
    "\n",
    "    for (img, save_filename) in zip(imgs, save_filenames):\n",
    "        save_image(img, f\"diffusion_{save_filename}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_patch(9, brightness=3.5)\n",
    "# imgs = save_single_patch(9, brightness=3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image(imgs[0].permute(2, 0, 1), \"diffusion_low_res.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwIw8ZjtZW5G"
   },
   "source": [
    "# Generating Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For clearing GPU memory\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9c2mPcxC3Jgq"
   },
   "outputs": [],
   "source": [
    "from super_resolution.src.testing import compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96abN028Qr_o"
   },
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_data, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMHoDuzYSthB"
   },
   "outputs": [],
   "source": [
    "compute_metrics(diffusionSR, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:stat3007-super-resolution] *",
   "language": "python",
   "name": "conda-env-stat3007-super-resolution-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
