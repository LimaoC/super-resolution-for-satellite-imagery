{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4HaVuWd04qM"
   },
   "source": [
    "# Super Resolution Diffusion Model Training Code\n",
    "## Group 6 Super Resolution Project\n",
    "\n",
    "Written following the guide at:\n",
    "\n",
    "https://huggingface.co/docs/diffusers/en/tutorials/basic_training\n",
    "\n",
    "and with reference to\n",
    "\n",
    "https://arxiv.org/pdf/2104.07636\n",
    "\n",
    "https://arxiv.org/pdf/2006.11239"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StLbGiquzrzI"
   },
   "source": [
    "## Training Code\n",
    "\n",
    "### Imports\n",
    "Make sure to install our package beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "C1VKJwEmys3K"
   },
   "outputs": [],
   "source": [
    "\n",
    "from super_resolution.src.sen2venus_dataset import S2VSite, S2VSites, create_train_test_split\n",
    "from super_resolution.src.visualization import plot_gallery\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import interpolate\n",
    "import py7zr as py7zr\n",
    "\n",
    "import diffusers\n",
    "import accelerate\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from diffusers import DDPMScheduler\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from diffusers import DDPMPipeline\n",
    "from diffusers import UNet2DModel\n",
    "\n",
    "\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from accelerate import notebook_launcher\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQf2JX-cz3N-"
   },
   "source": [
    "### Defining Training Configuration\n",
    "\n",
    "All the training parameters are set here for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "V08tfnPN0IFC"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Training Configuration\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 256  # the generated image resolution\n",
    "    train_batch_size = 12 # how many images to sample during training\n",
    "    num_epochs = 50\n",
    "    train_sites = set([\"SO2\"])\n",
    "    data_dir = str(\"/content/drive/MyDrive/STAT3007 Project Data/\")\n",
    "    output_dir = '/content/drive/MyDrive/STAT3007'\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_image_epochs = 10\n",
    "    save_model_epochs = 1\n",
    "    mixed_precision = 'fp16'  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
    "    seed = 0\n",
    "\n",
    "config = TrainingConfig()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGu0RyrWz-nQ"
   },
   "source": [
    "### Loading Data\n",
    "\n",
    "Just on SO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "rGICmhtv1brA"
   },
   "outputs": [],
   "source": [
    "def clamp_transform(x, y):\n",
    "    x = x[:3, :, :]\n",
    "    y = y[:3, :, :]\n",
    "\n",
    "    x = torch.clamp(x, 0, 1)\n",
    "    y = torch.clamp(y, 0, 1)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "train_data, test_data = create_train_test_split(\n",
    "    data_dir = config.data_dir,\n",
    "    seed = -1,\n",
    "    sites = config.train_sites,\n",
    "    device = config.device,\n",
    ")\n",
    "\n",
    "train_data.set_transform(clamp_transform)\n",
    "test_data.set_transform(clamp_transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=config.train_batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwcmeXdb0Jg0"
   },
   "source": [
    "### Defining Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "Q5J0zrhn0Nd3"
   },
   "outputs": [],
   "source": [
    "def normalize_tensor(tensor):\n",
    "    min_val = tensor.min()\n",
    "    max_val = tensor.max()\n",
    "    normalized_tensor = (tensor - min_val) / (max_val - min_val)\n",
    "    return normalized_tensor\n",
    "# # preprocessing\n",
    "process = nn.Sequential(\n",
    "    transforms.Lambda(normalize_tensor)\n",
    ")\n",
    "\n",
    "upscale = lambda x: interpolate(x, size=(256, 256), mode=\"bicubic\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTtBietJ0Q5n"
   },
   "source": [
    "### Defining the U-Net to be used\n",
    "\n",
    "Some fine tuning can be done here by modifying the underlying U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JIcDqgCM26oj"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Prior to Test 7\n",
    "# Test 8\n",
    "# model = UNet2DModel(\n",
    "#     sample_size=config.image_size,  # the target image resolution\n",
    "#     in_channels=6,  # 6 channels since the input is a concat of noise + upscaled low res\n",
    "#     out_channels=3,  # 3 RGB out channels\n",
    "#     layers_per_block=3,  # how many ResNet layers to use per UNet block\n",
    "#      # the number of output channels for each UNet block\n",
    "#     block_out_channels=(256, 256, 512, 512, 1024, 1024),\n",
    "#     down_block_types=(\n",
    "#         \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "#         \"DownBlock2D\",\n",
    "#         \"DownBlock2D\",\n",
    "#         \"DownBlock2D\",\n",
    "#         \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "#         \"DownBlock2D\",\n",
    "#     ),\n",
    "#     up_block_types=(\n",
    "#         \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "#         \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "#         \"UpBlock2D\",\n",
    "#         \"UpBlock2D\",\n",
    "#         \"UpBlock2D\",\n",
    "#         \"UpBlock2D\"\n",
    "#       ),\n",
    "# )\n",
    "\n",
    "# Prior to Test 7\n",
    "# Test 8\n",
    "model = UNet2DModel(\n",
    "    sample_size=config.image_size,  # the target image resolution\n",
    "    in_channels=6,  # 6 channels since the input is a concat of noise + upscaled low res\n",
    "    out_channels=3,  # 3 RGB out channels\n",
    "    layers_per_block=3,  # how many ResNet layers to use per UNet block\n",
    "     # the number of output channels for each UNet block\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\"\n",
    "      ),\n",
    ")\n",
    "# # Test 7\n",
    "# model = UNet2DModel(\n",
    "#     sample_size=config.image_size,  # the target image resolution\n",
    "#     in_channels=6,  # 6 channels since the input is a concat of noise + upscaled low res\n",
    "#     out_channels=3,  # 3 RGB out channels\n",
    "#     layers_per_block=3,  # how many ResNet layers to use per UNet block\n",
    "#      # the number of output channels for each UNet block\n",
    "#     block_out_channels=(128, 256, 256, 512, 512),\n",
    "#     down_block_types=(\n",
    "#         \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "#         \"DownBlock2D\",\n",
    "#         \"DownBlock2D\",\n",
    "#         \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "#         \"DownBlock2D\",\n",
    "#     ),\n",
    "#     up_block_types=(\n",
    "#         \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "#         \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "#         \"UpBlock2D\",\n",
    "#         \"UpBlock2D\",\n",
    "#         \"UpBlock2D\"\n",
    "#       ),\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Co3uVX0R0fMM"
   },
   "source": [
    "### Noise Pipelines and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KO33hPTf3B6K"
   },
   "outputs": [],
   "source": [
    "\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = config.learning_rate)\n",
    "\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(len(train_dataloader) * config.num_epochs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vQewu-10n6W"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQtyu4xk3XUF"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler, upscaler):\n",
    "    # Initialize accelerator and tensorboard logging\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        log_with=\"tensorboard\",\n",
    "        project_dir=os.path.join(config.output_dir, \"logs\"),\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        if config.output_dir is not None:\n",
    "            os.makedirs(config.output_dir, exist_ok=True)\n",
    "        accelerator.init_trackers(\"train_example\")\n",
    "\n",
    "    # Prepare the model and optimizer\n",
    "    model, optimizer, train_dataloader, lr_scheduler, upscaler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, lr_scheduler, upscaler\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    # Now you train the model\n",
    "    for epoch in range(config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            low, high = batch\n",
    "            upscaled = upscaler(low)\n",
    "\n",
    "            # Sample noise to add to the images\n",
    "            noise = torch.randn(high.shape, device=high.device)\n",
    "            bs = high.shape[0]\n",
    "\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(\n",
    "                0, noise_scheduler.config.num_train_timesteps, (bs,), device=high.device,\n",
    "                dtype=torch.int64\n",
    "            )\n",
    "\n",
    "            # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process)\n",
    "            noisy_images = noise_scheduler.add_noise(high, noise, timesteps)\n",
    "\n",
    "            with accelerator.accumulate(model):\n",
    "                # Predict the noise residual\n",
    "                noisy_images = torch.concat([noisy_images, upscaled], dim=1)\n",
    "                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "            global_step += 1\n",
    "\n",
    "        # After each epoch you optionally sample some demo images with evaluate() and save the model\n",
    "        if accelerator.is_main_process:\n",
    "            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)\n",
    "            pipeline.save_pretrained(config.output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LcmKXkCJ1uPt"
   },
   "source": [
    "### Launch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "og_ok9rq3Zji"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "args = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler, upscale)\n",
    "\n",
    "notebook_launcher(train_loop, args, num_processes=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JciPkh5G1Ju6"
   },
   "source": [
    "### Resume Training\n",
    "\n",
    "Doesn't work :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mH8sgxh1I8R"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = config.learning_rate)\n",
    "\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(len(train_dataloader) * config.num_epochs))\n",
    "\n",
    "pretrained_location = config.output_dir\n",
    "\n",
    "model = UNet2DModel.from_pretrained(pretrained_location + \"/unet\")\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(pretrained_location + \"/scheduler\")\n",
    "\n",
    "model.train()\n",
    "\n",
    "args = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler, upscale)\n",
    "\n",
    "notebook_launcher(train_loop, args, num_processes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiIDp85W18bM"
   },
   "source": [
    "## Testing Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNspQdqn2Drg"
   },
   "source": [
    "###  Load Model\n",
    "\n",
    "Will need to be configured to the location of the model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEH3WvBoCyGo"
   },
   "outputs": [],
   "source": [
    "model_location = config.output_dir\n",
    "loadModel = DDPMPipeline.from_pretrained(model_location)\n",
    "loadModel.to(config.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zr6sddhk2PA3"
   },
   "source": [
    "### Testing Code Loop\n",
    "Generates a High resolution sample from a low resolution input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r07Yxv36eHg4"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def test_diffuse(pipeline, upscaler, low, generator = None, process = None):\n",
    "\n",
    "    batch_size = low.shape[0]\n",
    "    noise = torch.randn(\n",
    "            (batch_size, 3, pipeline.unet.sample_size, pipeline.unet.sample_size),\n",
    "            generator=generator,\n",
    "        )\n",
    "    noise = noise.to(pipeline.device)\n",
    "\n",
    "    upscaled = upscaler(low)\n",
    "    upscaled = upscaled.to(pipeline.device)\n",
    "    # set step values\n",
    "    pipeline.scheduler.set_timesteps(1000)\n",
    "\n",
    "    for t in pipeline.progress_bar(pipeline.scheduler.timesteps):\n",
    "            # 1. predict noise model_output\n",
    "            image = torch.concat([noise, upscaled], dim=1)\n",
    "            model_output = pipeline.unet(image, t).sample\n",
    "\n",
    "            # 2. compute previous image: x_t -> t_t-1\n",
    "            noise = pipeline.scheduler.step(model_output, t, noise, generator=generator).prev_sample\n",
    "\n",
    "\n",
    "    # if process:\n",
    "    #     # noise = process(noise)\n",
    "    #     noise = process(noise / 2 + 0.5)\n",
    "    # else:\n",
    "    #     noise = (noise / 2 + 0.5).clamp(0, 1)\n",
    "    noise = noise.clamp(0,1)\n",
    "    # noise = noise.cpu().squeeze(0).permute(2,1,0)\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAb4Xla92Xal"
   },
   "source": [
    "### Generating Test Images\n",
    "\n",
    "To generate a single upscaled image use this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fu3WEU7AQ4Ox"
   },
   "outputs": [],
   "source": [
    "diffusionSR = lambda x: test_diffuse(loadModel, upscale, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QRz5tY8A3hzI"
   },
   "outputs": [],
   "source": [
    "# For clearing GPU memory\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNT-DIpW34Bl"
   },
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNx4bhLJI-vX"
   },
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_data, batch_size=1)\n",
    "NUM_IMAGES = 10\n",
    "\n",
    "test_imgs = []\n",
    "titles = []\n",
    "for (i, img) in enumerate(test_dataloader):\n",
    "  if i <= 10*(NUM_IMAGES - 1):\n",
    "    if i % 10 == 0:\n",
    "      titles.append(f\"Low Res {i}\")\n",
    "      titles.append(f\"Naive Upscale {i}\")\n",
    "      titles.append(f\"Upscaled {i}\")\n",
    "      titles.append(f\"High Res {i}\")\n",
    "      low, high = img\n",
    "      # low, high = process(low), process(high)\n",
    "      upscaled = test_diffuse(loadModel, upscale, low, process = process)\n",
    "      test_imgs.append(low.squeeze(0).permute(2,1,0))\n",
    "      test_imgs.append(upscale(low).squeeze(0).permute(2,1,0))\n",
    "      test_imgs.append(upscaled.squeeze(0).permute(2,1,0))\n",
    "      test_imgs.append(high.squeeze(0).permute(2,1,0))\n",
    "test_imgs = [t.cpu() for t in test_imgs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgrUzE_Rcs55"
   },
   "outputs": [],
   "source": [
    "plot_gallery(test_imgs, titles, nrow = NUM_IMAGES, xscale = 3, yscale = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwIw8ZjtZW5G"
   },
   "source": [
    "# Generating Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9c2mPcxC3Jgq"
   },
   "outputs": [],
   "source": [
    "from super_resolution.src.testing import compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96abN028Qr_o"
   },
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_data, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMHoDuzYSthB"
   },
   "outputs": [],
   "source": [
    "compute_metrics(diffusionSR, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWsDd68SaE-L"
   },
   "outputs": [],
   "source": [
    "# For clearing GPU memory\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
