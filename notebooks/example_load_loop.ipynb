{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from super_resolution.src.sen2venus_dataset import (\n",
    "    create_train_validation_test_split,\n",
    "    default_patch_transform,\n",
    ")\n",
    "\n",
    "from super_resolution.src.visualization import plot_gallery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path(\"C:/Users/skouf/Documents/2024/STAT3007/stat3007-project/SRCNN/data\")\n",
    "SITES_DIR = DATA_DIR / \"sites\"\n",
    "PREPROCESSING_DIR = DATA_DIR / \"preprocessing\"\n",
    "RESULTS_DIR = DATA_DIR / \"results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'py7zr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_patches, val_patches, test_patches \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_train_validation_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSITES_DIR\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msites\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFGMANAUS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNum train \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_patches)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNum validation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(val_patches)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNum test \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_patches)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m )\n",
      "File \u001b[1;32m~\\Documents\\2024\\STAT3007\\stat3007-project\\super_resolution\\src\\sen2venus_dataset.py:384\u001b[0m, in \u001b[0;36mcreate_train_validation_test_split\u001b[1;34m(data_dir, seed, sites, device)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_train_validation_test_split\u001b[39m(\n\u001b[0;32m    366\u001b[0m     data_dir: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    367\u001b[0m     seed: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    368\u001b[0m     sites: Optional[\u001b[38;5;28mset\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    369\u001b[0m     device: Union[torch\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    370\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[PatchData, PatchData, PatchData]:\n\u001b[0;32m    371\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create train-validation-test split using satellite data.\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m    Parameters:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;124;03m        test dataset tuple.\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 384\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_train_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msites\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msites\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m     cut_off \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(VAL_PROPORTION \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(test\u001b[38;5;241m.\u001b[39msamples))\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    389\u001b[0m         train,\n\u001b[0;32m    390\u001b[0m         PatchData(test\u001b[38;5;241m.\u001b[39msamples[:cut_off], device\u001b[38;5;241m=\u001b[39mdevice),\n\u001b[0;32m    391\u001b[0m         PatchData(test\u001b[38;5;241m.\u001b[39msamples[cut_off:], device\u001b[38;5;241m=\u001b[39mdevice),\n\u001b[0;32m    392\u001b[0m     )\n",
      "File \u001b[1;32m~\\Documents\\2024\\STAT3007\\stat3007-project\\super_resolution\\src\\sen2venus_dataset.py:342\u001b[0m, in \u001b[0;36mcreate_train_test_split\u001b[1;34m(data_dir, seed, sites, device)\u001b[0m\n\u001b[0;32m    340\u001b[0m site_samples \u001b[38;5;241m=\u001b[39m [[([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m sites]\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, site_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sites):\n\u001b[1;32m--> 342\u001b[0m     site \u001b[38;5;241m=\u001b[39m \u001b[43mS2VSite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43msite_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msite_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrgbnir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m     site_samples[i] \u001b[38;5;241m=\u001b[39m site\u001b[38;5;241m.\u001b[39msamples\n\u001b[0;32m    349\u001b[0m all_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(site_samples))\n",
      "File \u001b[1;32m~\\Documents\\2024\\STAT3007\\stat3007-project\\super_resolution\\src\\sen2venus_dataset.py:135\u001b[0m, in \u001b[0;36mS2VSite.__init__\u001b[1;34m(self, site_name, bands, download_dir, device)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Download and extract the dataset (if it hasn't already been downloaded)\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# Parse samples from the extracted dataset\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_samples()\n",
      "File \u001b[1;32m~\\Documents\\2024\\STAT3007\\stat3007-project\\super_resolution\\src\\sen2venus_dataset.py:239\u001b[0m, in \u001b[0;36mS2VSite.download_and_extract\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    232\u001b[0m     download_url(\n\u001b[0;32m    233\u001b[0m         url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    234\u001b[0m         root\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_dir,\n\u001b[0;32m    235\u001b[0m         md5\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m    236\u001b[0m         filename\u001b[38;5;241m=\u001b[39mzip_name,\n\u001b[0;32m    237\u001b[0m     )\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_extracted():\n\u001b[1;32m--> 239\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mpy7zr\u001b[49m\u001b[38;5;241m.\u001b[39mSevenZipFile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_dir \u001b[38;5;241m+\u001b[39m zip_name, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;28mzip\u001b[39m:\n\u001b[0;32m    240\u001b[0m         \u001b[38;5;28mzip\u001b[39m\u001b[38;5;241m.\u001b[39mextractall(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_dir)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'py7zr' is not defined"
     ]
    }
   ],
   "source": [
    "train_patches, val_patches, test_patches = create_train_validation_test_split(\n",
    "    str(SITES_DIR) + \"\\\\\", sites={\"FGMANAUS\"}\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Num train {len(train_patches)}\\n\"\n",
    "    f\"Num validation {len(val_patches)}\\n\"\n",
    "    f\"Num test {len(test_patches)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_patches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[43mtrain_patches\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_patches' is not defined"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_patches, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (low_res, high_res) in enumerate(train_loader):\n",
    "    print(\n",
    "        f\"batch {i}\\n\"\n",
    "        f\"low resolution batch shape {low_res.shape}\\n\"\n",
    "        f\"high resolution batch shape {high_res.shape}\\n\"\n",
    "    )\n",
    "\n",
    "    if i == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_res_example = low_res[0]\n",
    "high_res_example = high_res[0]\n",
    "plot_gallery(\n",
    "    [low_res_example.permute(1, 2, 0), high_res_example.permute(1, 2, 0)],\n",
    "    xscale=5,\n",
    "    yscale=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading with transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often want to apply some transforms calculated from our training set and apply these\n",
    "to both the training set and test sets before running data through our model. These custom transforms\n",
    "are useful for training stability and normalization. This gives us another component of the model we\n",
    "can experiment with if we choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~2min\n",
    "# Compute mean and standard deviation for each channel (averaged over samples).\n",
    "train_loader = DataLoader(train_patches, batch_size=1000)\n",
    "\n",
    "mean = 0.0\n",
    "std = 0.0\n",
    "for i, (low_res, _) in enumerate(train_loader):\n",
    "    mean += torch.sum(torch.mean(low_res, (2, 3)), 0)\n",
    "    std += torch.sum(torch.std(low_res, (2, 3)), 0)\n",
    "mean /= len(train_patches)\n",
    "std /= len(train_patches)\n",
    "print(f\"Channel means {mean}\\n\" f\"Channel standard deviations {std}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the mean and std can be slow. Here is some code for saving and loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = PREPROCESSING_DIR / \"K34-AMAZ_mean_std.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(save_file, \"wb\") as file:\n",
    "#     pickle.dump((mean, std), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(save_file, \"rb\") as file:\n",
    "#     mean, std = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_transform(\n",
    "    low_res_patch: torch.Tensor, high_res_patch: torch.Tensor\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    augmentations = transforms.Compose(\n",
    "        [\n",
    "            transforms.Normalize(mean, std),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Basic transforms such as removing 4th channel and scaling surface reflectants\n",
    "    low_res_patch, high_res_patch = default_patch_transform(\n",
    "        low_res_patch, high_res_patch\n",
    "    )\n",
    "\n",
    "    # torchvision transforms expects shape (CxHxW) so permute accordingly\n",
    "    low_res_patch = low_res_patch.permute(0, 2, 1)\n",
    "    low_res_augmented = augmentations(low_res_patch).permute(0, 2, 1)\n",
    "\n",
    "    return low_res_augmented, high_res_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_patches.set_transform(custom_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images will now be loaded with the above custom transform applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(3, 3, 3, 1, 65)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleSR()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Loop Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "should_pin_memory = {\"cuda\": True, \"cpu\": False}\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_patches,\n",
    "    shuffle=True,  # Always set to true for training\n",
    "    batch_size=128,  # Always try to set as powers of 2\n",
    "    drop_last=True,  # Ensures batch size is always the one given (Drops last if its is smaller)\n",
    "    pin_memory=should_pin_memory[device.type],  # Faster push to GPU\n",
    "    num_workers=2,  # Load data in parallel but costs more memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~21min for 10 epochs\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "losses = []\n",
    "epoch_losses = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    progress_bar = tqdm.tqdm(train_loader, total=len(train_loader), ncols=100)\n",
    "    epoch_loss = 0.0\n",
    "    for low_res_batch, high_res_batch in progress_bar:\n",
    "        # Push to GPU\n",
    "        low_res_batch = low_res_batch.to(device)\n",
    "        high_res_batch = high_res_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Loss and update\n",
    "        out = model(low_res_batch)\n",
    "        loss = criterion(out, high_res_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Collect data\n",
    "        epoch_loss += loss.item()\n",
    "        losses.append(loss.item())\n",
    "        progress_bar.set_postfix(epoch=epoch, batch_loss=loss.item())\n",
    "\n",
    "    epoch_loss /= len(train_loader)\n",
    "    epoch_losses.append(epoch_loss)\n",
    "    print(f\"Epoch: {epoch} / loss: {epoch_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = {\n",
    "    \"epoch_losses\": epoch_losses,\n",
    "    \"batch_losses\": losses,\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"optimizer_state\": optimizer.state_dict(),\n",
    "}\n",
    "save_file = RESULTS_DIR / \"basic_sr_results.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(save_file, \"wb\") as file:\n",
    "#     pickle.dump(experiment, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(save_file, \"rb\") as file:\n",
    "#     experiment = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.title(\"Losses for each minibatch\")\n",
    "plt.xlabel(\"Minibatch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_losses)\n",
    "plt.title(\"Losses for each epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Mean MSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(low_res_example.unsqueeze(0)).squeeze(0)\n",
    "out = (out - out.min()) / (out.max() - out.min())\n",
    "plot_gallery(\n",
    "    [\n",
    "        low_res_example.permute(1, 2, 0),\n",
    "        high_res_example.permute(1, 2, 0),\n",
    "        out.permute(1, 2, 0),\n",
    "    ],\n",
    "    xscale=5,\n",
    "    yscale=5,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "getafix_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
