{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mitch\\miniconda3\\envs\\getafix_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Inspired by https://github.com/ioangatop/srVAE\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01msrVAE\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m      7\u001b[0m         x_shape,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m         device\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[0;32m     13\u001b[0m     ):\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m, in \u001b[0;36msrVAE\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01msrVAE\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m      7\u001b[0m         x_shape,\n\u001b[0;32m      8\u001b[0m         y_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m16\u001b[39m),\n\u001b[1;32m----> 9\u001b[0m         u_dim\u001b[38;5;241m=\u001b[39m\u001b[43margs\u001b[49m\u001b[38;5;241m.\u001b[39mu_dim,\n\u001b[0;32m     10\u001b[0m         z_dim\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mz_dim,\n\u001b[0;32m     11\u001b[0m         prior\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mprior,\n\u001b[0;32m     12\u001b[0m         device\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[0;32m     13\u001b[0m     ):\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mdevice\n",
      "\u001b[1;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "# Inspired by https://github.com/ioangatop/srVAE\n",
    "\n",
    "\n",
    "class srVAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_shape,\n",
    "        y_shape=(3, 16, 16),\n",
    "        u_dim=args.u_dim,\n",
    "        z_dim=args.z_dim,\n",
    "        prior=args.prior,\n",
    "        device=args.device,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = args.device\n",
    "        self.x_shape = x_shape\n",
    "        self.y_shape = (x_shape[0], y_shape[1], y_shape[2])\n",
    "\n",
    "        self.u_shape = get_shape(u_dim)\n",
    "        self.z_shape = get_shape(z_dim)\n",
    "\n",
    "        # q(y|x): deterministic \"compressed\" transformation\n",
    "        self.compressed_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((self.y_shape[1], self.y_shape[2])),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # p(u)\n",
    "        self.p_u = globals()[prior](self.u_shape)\n",
    "\n",
    "        # q(u | y)\n",
    "        self.q_u = q_u(self.u_shape, self.y_shape)\n",
    "\n",
    "        # p(z | y)\n",
    "        self.p_z = p_z(self.z_shape, (self.y_shape, self.u_shape))\n",
    "\n",
    "        # q(z | x)\n",
    "        self.q_z = q_z(self.z_shape, self.x_shape)\n",
    "\n",
    "        # p(y | u)\n",
    "        self.p_y = p_y(self.y_shape, self.u_shape)\n",
    "\n",
    "        # p(x | y, z)\n",
    "        self.p_x = p_x(self.x_shape, (self.y_shape, self.z_shape))\n",
    "\n",
    "        # likelihood distribution\n",
    "        self.recon_loss = partial(dmol_loss)\n",
    "        self.sample_distribution = partial(sample_from_dmol)\n",
    "\n",
    "    def compressed_transoformation(self, input):\n",
    "        y = []\n",
    "        for x in input:\n",
    "            y.append(self.compressed_transform(x.cpu()))\n",
    "        return torch.stack(y).to(self.device)\n",
    "\n",
    "    def initialize(self, dataloader):\n",
    "        \"\"\"Data dependent init for weight normalization\n",
    "        (Automatically done during the first forward pass).\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            x, _ = next(iter(dataloader))\n",
    "            x = x.to(self.device)\n",
    "            output = self.forward(x)\n",
    "            self.calculate_elbo(x, output)\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def reparameterize(z_mean, z_log_var):\n",
    "        \"\"\"z ~ N(z| z_mu, z_logvar)\"\"\"\n",
    "        epsilon = torch.randn_like(z_mean)\n",
    "        return z_mean + torch.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, n_samples=20):\n",
    "        # u ~ p(u)\n",
    "        u = self.p_u.sample(self.u_shape, n_samples=n_samples, device=self.device).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        # p(y|u)\n",
    "        y_logits = self.p_y(u)\n",
    "        y_hat = self.sample_distribution(y_logits, nc=self.y_shape[0])\n",
    "\n",
    "        # z ~ p(z|y, u)\n",
    "        z_p_mean, z_p_logvar = self.p_z((y_hat, u))\n",
    "        z_p = self.reparameterize(z_p_mean, z_p_logvar)\n",
    "\n",
    "        # x ~ p(x|y,z)\n",
    "        x_logits = self.p_x((y_hat, z_p))\n",
    "        x_hat = self.sample_distribution(x_logits, nc=self.x_shape[0])\n",
    "        return x_hat, y_hat\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def reconstruct(self, x, **kwargs):\n",
    "        outputs = self.forward(x)\n",
    "        y_hat = self.sample_distribution(outputs.get(\"y_logits\"), nc=self.y_shape[0])\n",
    "        x_hat = self.sample_distribution(outputs.get(\"x_logits\"), nc=self.x_shape[0])\n",
    "        return outputs.get(\"y\"), y_hat, x_hat\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def super_resolution(self, y):\n",
    "        # u ~ q(u| y)\n",
    "        u_q_mean, u_q_logvar = self.q_u(y)\n",
    "        u_q = self.reparameterize(u_q_mean, u_q_logvar)\n",
    "\n",
    "        # z ~ p(z|y)\n",
    "        z_p_mean, z_p_logvar = self.p_z((y, u_q))\n",
    "        z_p = self.reparameterize(z_p_mean, z_p_logvar)\n",
    "\n",
    "        # x ~ p(x|y,z)\n",
    "        x_logits = self.p_x((y, z_p))\n",
    "        x_hat = self.sample_distribution(x_logits)\n",
    "        return x_hat\n",
    "\n",
    "    def calculate_elbo(self, x, outputs, **kwargs):\n",
    "        # unpack variables\n",
    "        y, x_logits, y_logits = (\n",
    "            outputs.get(\"y\"),\n",
    "            outputs.get(\"x_logits\"),\n",
    "            outputs.get(\"y_logits\"),\n",
    "        )\n",
    "        u_q, u_q_mean, u_q_logvar = (\n",
    "            outputs.get(\"u_q\"),\n",
    "            outputs.get(\"u_q_mean\"),\n",
    "            outputs.get(\"u_q_logvar\"),\n",
    "        )\n",
    "        z_q, z_q_mean, z_q_logvar = (\n",
    "            outputs.get(\"z_q\"),\n",
    "            outputs.get(\"z_q_mean\"),\n",
    "            outputs.get(\"z_q_logvar\"),\n",
    "        )\n",
    "        z_p_mean, z_p_logvar = outputs.get(\"z_p_mean\"), outputs.get(\"z_p_logvar\")\n",
    "\n",
    "        # Reconstraction loss\n",
    "        RE_x = self.recon_loss(x, x_logits, nc=self.x_shape[0])\n",
    "        RE_y = self.recon_loss(y, y_logits, nc=self.y_shape[0])\n",
    "\n",
    "        # Regularization loss\n",
    "        log_p_u = self.p_u.log_p(u_q, dim=1)\n",
    "        log_q_u = log_normal_diag(u_q, u_q_mean, u_q_logvar)\n",
    "        KL_u = log_q_u - log_p_u\n",
    "\n",
    "        log_p_z = log_normal_diag(z_q, z_p_mean, z_p_logvar)\n",
    "        log_q_z = log_normal_diag(z_q, z_q_mean, z_q_logvar)\n",
    "        KL_z = log_q_z - log_p_z\n",
    "\n",
    "        # Total lower bound loss\n",
    "        nelbo = -(RE_x + RE_y - KL_u - KL_z).mean()\n",
    "\n",
    "        diagnostics = {\n",
    "            \"bpd\": (nelbo.item()) / (np.prod(x.shape[1:]) * np.log(2.0)),\n",
    "            \"nelbo\": nelbo.item(),\n",
    "            \"RE\": -(RE_x + RE_y).mean().item(),\n",
    "            \"RE_x\": -RE_x.mean().item(),\n",
    "            \"RE_y\": -RE_y.mean().item(),\n",
    "            \"KL\": (KL_z + KL_u).mean().item(),\n",
    "            \"KL_u\": KL_u.mean().item(),\n",
    "            \"KL_z\": KL_z.mean().item(),\n",
    "        }\n",
    "        return nelbo, diagnostics\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        \"\"\"Forward pass through the inference and the generative model.\"\"\"\n",
    "        # y ~ f(x) (determinist)\n",
    "        y = self.compressed_transoformation(x)\n",
    "\n",
    "        # u ~ q(u| y)\n",
    "        u_q_mean, u_q_logvar = self.q_u(y)\n",
    "        u_q = self.reparameterize(u_q_mean, u_q_logvar)\n",
    "\n",
    "        # z ~ q(z| x, y)\n",
    "        z_q_mean, z_q_logvar = self.q_z(x)\n",
    "        z_q = self.reparameterize(z_q_mean, z_q_logvar)\n",
    "\n",
    "        # x ~ p(x| y, z)\n",
    "        x_logits = self.p_x((y, z_q))\n",
    "\n",
    "        # y ~ p(y| u)\n",
    "        y_logits = self.p_y(u_q)\n",
    "\n",
    "        # z ~ p(z| x)\n",
    "        z_p_mean, z_p_logvar = self.p_z((y, u_q))\n",
    "\n",
    "        return {\n",
    "            \"u_q_mean\": u_q_mean,\n",
    "            \"u_q_logvar\": u_q_logvar,\n",
    "            \"u_q\": u_q,\n",
    "            \"z_q_mean\": z_q_mean,\n",
    "            \"z_q_logvar\": z_q_logvar,\n",
    "            \"z_q\": z_q,\n",
    "            \"z_p_mean\": z_p_mean,\n",
    "            \"z_p_logvar\": z_p_logvar,\n",
    "            \"y\": y,\n",
    "            \"y_logits\": y_logits,\n",
    "            \"x_logits\": x_logits,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "getafix_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
