{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by https://github.com/ioangatop/srVAE\n",
    "\n",
    "\n",
    "class srVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Super-Resolution Variational Auto-Encoder (srVAE).\n",
    "    A Two Staged Visual Processing Variational AutoEncoder.\n",
    "\n",
    "    Author:\n",
    "    Ioannis Gatopoulos.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_shape,\n",
    "        y_shape=(3, 16, 16),\n",
    "        u_dim=args.u_dim,\n",
    "        z_dim=args.z_dim,\n",
    "        prior=args.prior,\n",
    "        device=args.device,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = args.device\n",
    "        self.x_shape = x_shape\n",
    "        self.y_shape = (x_shape[0], y_shape[1], y_shape[2])\n",
    "\n",
    "        self.u_shape = get_shape(u_dim)\n",
    "        self.z_shape = get_shape(z_dim)\n",
    "\n",
    "        # q(y|x): deterministic \"compressed\" transformation\n",
    "        self.compressed_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((self.y_shape[1], self.y_shape[2])),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # p(u)\n",
    "        self.p_u = globals()[prior](self.u_shape)\n",
    "\n",
    "        # q(u | y)\n",
    "        self.q_u = q_u(self.u_shape, self.y_shape)\n",
    "\n",
    "        # p(z | y)\n",
    "        self.p_z = p_z(self.z_shape, (self.y_shape, self.u_shape))\n",
    "\n",
    "        # q(z | x)\n",
    "        self.q_z = q_z(self.z_shape, self.x_shape)\n",
    "\n",
    "        # p(y | u)\n",
    "        self.p_y = p_y(self.y_shape, self.u_shape)\n",
    "\n",
    "        # p(x | y, z)\n",
    "        self.p_x = p_x(self.x_shape, (self.y_shape, self.z_shape))\n",
    "\n",
    "        # likelihood distribution\n",
    "        self.recon_loss = partial(dmol_loss)\n",
    "        self.sample_distribution = partial(sample_from_dmol)\n",
    "\n",
    "    def compressed_transoformation(self, input):\n",
    "        y = []\n",
    "        for x in input:\n",
    "            y.append(self.compressed_transform(x.cpu()))\n",
    "        return torch.stack(y).to(self.device)\n",
    "\n",
    "    def initialize(self, dataloader):\n",
    "        \"\"\"Data dependent init for weight normalization\n",
    "        (Automatically done during the first forward pass).\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            x, _ = next(iter(dataloader))\n",
    "            x = x.to(self.device)\n",
    "            output = self.forward(x)\n",
    "            self.calculate_elbo(x, output)\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def reparameterize(z_mean, z_log_var):\n",
    "        \"\"\"z ~ N(z| z_mu, z_logvar)\"\"\"\n",
    "        epsilon = torch.randn_like(z_mean)\n",
    "        return z_mean + torch.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, n_samples=20):\n",
    "        # u ~ p(u)\n",
    "        u = self.p_u.sample(self.u_shape, n_samples=n_samples, device=self.device).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        # p(y|u)\n",
    "        y_logits = self.p_y(u)\n",
    "        y_hat = self.sample_distribution(y_logits, nc=self.y_shape[0])\n",
    "\n",
    "        # z ~ p(z|y, u)\n",
    "        z_p_mean, z_p_logvar = self.p_z((y_hat, u))\n",
    "        z_p = self.reparameterize(z_p_mean, z_p_logvar)\n",
    "\n",
    "        # x ~ p(x|y,z)\n",
    "        x_logits = self.p_x((y_hat, z_p))\n",
    "        x_hat = self.sample_distribution(x_logits, nc=self.x_shape[0])\n",
    "        return x_hat, y_hat\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def reconstruct(self, x, **kwargs):\n",
    "        outputs = self.forward(x)\n",
    "        y_hat = self.sample_distribution(outputs.get(\"y_logits\"), nc=self.y_shape[0])\n",
    "        x_hat = self.sample_distribution(outputs.get(\"x_logits\"), nc=self.x_shape[0])\n",
    "        return outputs.get(\"y\"), y_hat, x_hat\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def super_resolution(self, y):\n",
    "        # u ~ q(u| y)\n",
    "        u_q_mean, u_q_logvar = self.q_u(y)\n",
    "        u_q = self.reparameterize(u_q_mean, u_q_logvar)\n",
    "\n",
    "        # z ~ p(z|y)\n",
    "        z_p_mean, z_p_logvar = self.p_z((y, u_q))\n",
    "        z_p = self.reparameterize(z_p_mean, z_p_logvar)\n",
    "\n",
    "        # x ~ p(x|y,z)\n",
    "        x_logits = self.p_x((y, z_p))\n",
    "        x_hat = self.sample_distribution(x_logits)\n",
    "        return x_hat\n",
    "\n",
    "    def calculate_elbo(self, x, outputs, **kwargs):\n",
    "        # unpack variables\n",
    "        y, x_logits, y_logits = (\n",
    "            outputs.get(\"y\"),\n",
    "            outputs.get(\"x_logits\"),\n",
    "            outputs.get(\"y_logits\"),\n",
    "        )\n",
    "        u_q, u_q_mean, u_q_logvar = (\n",
    "            outputs.get(\"u_q\"),\n",
    "            outputs.get(\"u_q_mean\"),\n",
    "            outputs.get(\"u_q_logvar\"),\n",
    "        )\n",
    "        z_q, z_q_mean, z_q_logvar = (\n",
    "            outputs.get(\"z_q\"),\n",
    "            outputs.get(\"z_q_mean\"),\n",
    "            outputs.get(\"z_q_logvar\"),\n",
    "        )\n",
    "        z_p_mean, z_p_logvar = outputs.get(\"z_p_mean\"), outputs.get(\"z_p_logvar\")\n",
    "\n",
    "        # Reconstraction loss\n",
    "        RE_x = self.recon_loss(x, x_logits, nc=self.x_shape[0])\n",
    "        RE_y = self.recon_loss(y, y_logits, nc=self.y_shape[0])\n",
    "\n",
    "        # Regularization loss\n",
    "        log_p_u = self.p_u.log_p(u_q, dim=1)\n",
    "        log_q_u = log_normal_diag(u_q, u_q_mean, u_q_logvar)\n",
    "        KL_u = log_q_u - log_p_u\n",
    "\n",
    "        log_p_z = log_normal_diag(z_q, z_p_mean, z_p_logvar)\n",
    "        log_q_z = log_normal_diag(z_q, z_q_mean, z_q_logvar)\n",
    "        KL_z = log_q_z - log_p_z\n",
    "\n",
    "        # Total lower bound loss\n",
    "        nelbo = -(RE_x + RE_y - KL_u - KL_z).mean()\n",
    "\n",
    "        diagnostics = {\n",
    "            \"bpd\": (nelbo.item()) / (np.prod(x.shape[1:]) * np.log(2.0)),\n",
    "            \"nelbo\": nelbo.item(),\n",
    "            \"RE\": -(RE_x + RE_y).mean().item(),\n",
    "            \"RE_x\": -RE_x.mean().item(),\n",
    "            \"RE_y\": -RE_y.mean().item(),\n",
    "            \"KL\": (KL_z + KL_u).mean().item(),\n",
    "            \"KL_u\": KL_u.mean().item(),\n",
    "            \"KL_z\": KL_z.mean().item(),\n",
    "        }\n",
    "        return nelbo, diagnostics\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        \"\"\"Forward pass through the inference and the generative model.\"\"\"\n",
    "        # y ~ f(x) (determinist)\n",
    "        y = self.compressed_transoformation(x)\n",
    "\n",
    "        # u ~ q(u| y)\n",
    "        u_q_mean, u_q_logvar = self.q_u(y)\n",
    "        u_q = self.reparameterize(u_q_mean, u_q_logvar)\n",
    "\n",
    "        # z ~ q(z| x, y)\n",
    "        z_q_mean, z_q_logvar = self.q_z(x)\n",
    "        z_q = self.reparameterize(z_q_mean, z_q_logvar)\n",
    "\n",
    "        # x ~ p(x| y, z)\n",
    "        x_logits = self.p_x((y, z_q))\n",
    "\n",
    "        # y ~ p(y| u)\n",
    "        y_logits = self.p_y(u_q)\n",
    "\n",
    "        # z ~ p(z| x)\n",
    "        z_p_mean, z_p_logvar = self.p_z((y, u_q))\n",
    "\n",
    "        return {\n",
    "            \"u_q_mean\": u_q_mean,\n",
    "            \"u_q_logvar\": u_q_logvar,\n",
    "            \"u_q\": u_q,\n",
    "            \"z_q_mean\": z_q_mean,\n",
    "            \"z_q_logvar\": z_q_logvar,\n",
    "            \"z_q\": z_q,\n",
    "            \"z_p_mean\": z_p_mean,\n",
    "            \"z_p_logvar\": z_p_logvar,\n",
    "            \"y\": y,\n",
    "            \"y_logits\": y_logits,\n",
    "            \"x_logits\": x_logits,\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
