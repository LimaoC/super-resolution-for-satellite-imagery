{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import interpolate\n",
    "from torcheval.metrics.functional import peak_signal_noise_ratio\n",
    "from torchmetrics.functional.image import structural_similarity_index_measure\n",
    "\n",
    "from super_resolution.src.sen2venus_dataset import (\n",
    "    S2VSite,\n",
    "    S2VSites,\n",
    "    create_train_test_split,\n",
    ")\n",
    "from super_resolution.src.visualization import plot_gallery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path(\"C:/Users/Mitch/stat3007_data\")\n",
    "SITES_DIR = DATA_DIR / \"sites\"\n",
    "BICUBIC_DIR = DATA_DIR / \"bicubic_results\"\n",
    "\n",
    "TOTAL_SAMPLES = 132_955"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test standard metrics on bicubic interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = S2VSite(\n",
    "    site_name=\"FGMANAUS\",\n",
    "    bands=\"rgbnir\",\n",
    "    download_dir=str(DATA_DIR / \"sites\") + \"\\\\\",\n",
    "    device=\"cpu\",\n",
    ")\n",
    "print(f\"{len(site)} patches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_images(images: torch.Tensor) -> torch.Tensor:\n",
    "    min_val = images.min()\n",
    "    max_val = images.max()\n",
    "\n",
    "    return (images - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "def preprocess_images(images: torch.Tensor, scale_output: bool = True) -> torch.Tensor:\n",
    "    images = images[:, :3, :, :]\n",
    "    if scale_output:\n",
    "        images = scale_images(images)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocess_images(\n",
    "    torch.concat([sentinal_image.unsqueeze(0) for sentinal_image, _ in site]),\n",
    "    scale_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_target = preprocess_images(\n",
    "    torch.concat([venus_image.unsqueeze(0) for _, venus_image in site]),\n",
    "    scale_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_X = interpolate(X, size=(256, 256), mode=\"bicubic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr = peak_signal_noise_ratio(interpolated_X, Y_target)\n",
    "ssim = structural_similarity_index_measure(interpolated_X, Y_target)\n",
    "print(f\"Metrics\\nPSNR: {psnr}\\nSSIM: {ssim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display example\n",
    "spacing = 23\n",
    "space = lambda num_space: num_space * \" \"\n",
    "print(\n",
    "    f\"{space(spacing)}Sentinel{space(2*spacing)}Bicubic{space(2*spacing)}Venus{space(spacing)}\"\n",
    ")\n",
    "for i in range(1):\n",
    "    plot_gallery(\n",
    "        [\n",
    "            scale_images(X[i].permute(1, 2, 0)),\n",
    "            scale_images(interpolated_X[i].permute(1, 2, 0)),\n",
    "            scale_images(Y_target[i].permute(1, 2, 0)),\n",
    "        ],\n",
    "        xscale=10,\n",
    "        yscale=10,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Speed of Bicubic Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "NUM_SITES_TO_TRY = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_names = [site_name for site_name, _ in S2VSites.SITES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run bicubic interpolation on a cumulative number of sites.\n",
    "cum_num_samples = []\n",
    "cum_time = []\n",
    "for i in range(NUM_SITES_TO_TRY):\n",
    "    num_samples = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Prepare data\n",
    "    train_data, test_data = create_train_test_split(\n",
    "        str(DATA_DIR / \"sites\") + \"\\\\\", seed=42, sites=set(site_names[: i + 1])\n",
    "    )\n",
    "    train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Start interpolatin\n",
    "    for X, Y in train_dataloader:\n",
    "        interpolated_X = interpolate(X, size=(256, 256), mode=\"bicubic\")\n",
    "        num_samples += X.shape[0]\n",
    "        cum_num_samples.append(num_samples)\n",
    "        cum_time.append(time.time() - start_time)\n",
    "    for X, Y in test_dataloader:\n",
    "        interpolated_X = interpolate(X, size=(256, 256), mode=\"bicubic\")\n",
    "        num_samples += X.shape[0]\n",
    "        cum_num_samples.append(num_samples)\n",
    "        cum_time.append(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"samples2500_batchsize512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_file = FILE_NAME + \"_cumsamples.pkl\"\n",
    "time_file = FILE_NAME + \"_cumtimes.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results from the long running process as a pkl file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(BICUBIC_DIR / samples_file, \"wb\") as file:\n",
    "#     pickle.dump(cum_num_samples, file)\n",
    "\n",
    "# with open(BICUBIC_DIR / time_file, \"wb\") as file:\n",
    "#     pickle.dump(cum_time, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BICUBIC_DIR / samples_file, \"rb\") as file:\n",
    "    cum_num_samples = pickle.load(file)\n",
    "\n",
    "with open(BICUBIC_DIR / time_file, \"rb\") as file:\n",
    "    cum_time = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cum_num_samples, cum_time)\n",
    "plt.xlabel(\"Number of samples\")\n",
    "plt.ylabel(\"Time to complete bicubic interpolation (seconds)\")\n",
    "plt.title(\"Speed performance of bicubic interpolation on CPU\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolate time on all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(\n",
    "    np.expand_dims(np.array(cum_num_samples), 1), np.expand_dims(np.array(cum_time), 1)\n",
    ")\n",
    "num_samples = np.expand_dims(np.linspace(0, 132_955), 1)\n",
    "predicted_times = lr.predict(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(num_samples, predicted_times / 60**2, label=\"Predicted times\")\n",
    "plt.plot(cum_num_samples, np.array(cum_time) / 60**2, label=\"True times\")\n",
    "plt.xlabel(\"Number of samples\")\n",
    "plt.ylabel(\"Time to complete bicubic interpolation (hours)\")\n",
    "plt.title(\"Speed performance of bicubic interpolation on CPU\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_time = lr.predict(np.array([[TOTAL_SAMPLES]]))\n",
    "print(\n",
    "    f\"Predicted {predicted_time[0, 0] / 60 / 60:.2f}hrs for running bicubic interpolation on all samples\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
