{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.nn.functional import interpolate\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms.functional import adjust_brightness\n",
    "\n",
    "from super_resolution.src.sen2venus_dataset import (\n",
    "    create_train_test_split,\n",
    ")\n",
    "from super_resolution.src.visualization import plot_gallery\n",
    "from super_resolution.src.srgan import SRResNet, Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path(\"C:/Users/Mitch/stat3007_data\")\n",
    "SITES_DIR = DATA_DIR / \"sites\"\n",
    "PREPROCESSING_DIR = DATA_DIR / \"preprocessing\"\n",
    "RESULTS_DIR = DATA_DIR / \"results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = {\n",
    "    \"SO2\",\n",
    "    \"FR-BIL\",\n",
    "    \"NARYN\",\n",
    "}\n",
    "train_patches, test_patches = create_train_test_split(\n",
    "    str(SITES_DIR) + \"\\\\\", sites=sites\n",
    ")\n",
    "print(f\"Num train {len(train_patches)}\\n\" f\"Num test {len(test_patches)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_transform(x, y):\n",
    "    x = x[:3, :, :]\n",
    "    y = y[:3, :, :]\n",
    "\n",
    "    x = torch.clamp(x, 0, 1)\n",
    "    y = torch.clamp(y, 0, 1)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_patches.set_transform(image_transform)\n",
    "test_patches.set_transform(image_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_patches, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(low_res, high_res) = next(train_loader.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = random.randint(0, len(low_res) - 1)\n",
    "low_res_example = low_res[index]\n",
    "\n",
    "\n",
    "high_res_example = high_res[index]\n",
    "\n",
    "\n",
    "plot_gallery(\n",
    "    [\n",
    "        adjust_brightness(low_res_example, 2).permute(1, 2, 0),\n",
    "        adjust_brightness(high_res_example, 2).permute(1, 2, 0),\n",
    "    ],\n",
    "    titles=[\"low res\", \"high res\"],\n",
    "    xscale=5,\n",
    "    yscale=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For clearing GPU memory\n",
    "# 1 / 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For clearing GPU memory\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "GEN_LEARNING_RATE = 1e-5\n",
    "DISCRIM_LEARNING_RATE = 1e-5\n",
    "GEN_WEIGHT_DECAY = 1e-8\n",
    "DISCRIM_WEIGHT_DECAY = 1e-8\n",
    "DISCRIM_WEIGHT = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "should_pin_memory = {\"cuda\": True, \"cpu\": False}\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_patches,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    drop_last=True,\n",
    "    pin_memory=should_pin_memory[device.type],\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = SRResNet(scaling_factor=2, n_blocks=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator(n_blocks=2, fc_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator.to(device)\n",
    "generator = generator.train()\n",
    "discriminator = discriminator.to(device)\n",
    "discriminator = discriminator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_optimizer = optim.Adam(\n",
    "    generator.parameters(), lr=GEN_LEARNING_RATE, weight_decay=GEN_WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "discrim_optimizer = optim.Adam(\n",
    "    discriminator.parameters(),\n",
    "    lr=DISCRIM_LEARNING_RATE,\n",
    "    weight_decay=DISCRIM_WEIGHT_DECAY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_file = RESULTS_DIR / \"continuingbestgen_374epoch.pkl\"\n",
    "# loaded_experiment = torch.load(save_file, map_location=device)\n",
    "# # generator = SRResNet(scaling_factor=2, n_blocks=16).to(device)\n",
    "# # discriminator = Discriminator(n_blocks=3, fc_size=128)\n",
    "# generator.load_state_dict(loaded_experiment[\"gen_state\"])\n",
    "# discriminator.load_state_dict(loaded_experiment[\"discrim_state\"])\n",
    "# gen_optimizer.load_state_dict(loaded_experiment[\"gen_optimizer_state\"])\n",
    "# discrim_optimizer.load_state_dict(loaded_experiment[\"discrim_optimizer_state\"])\n",
    "# gen_losses = loaded_experiment[\"gen_losses\"]\n",
    "# discrim_losses = loaded_experiment[\"discrim_losses\"]\n",
    "# train_time = loaded_experiment[\"train_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_criterion = nn.MSELoss()\n",
    "discrim_criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_time = 0.0\n",
    "gen_losses = []\n",
    "discrim_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator.to(device)\n",
    "generator = generator.train()\n",
    "discriminator = discriminator.to(device)\n",
    "discriminator = discriminator.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    progress_bar = tqdm.tqdm(train_loader, total=len(train_loader), ncols=100)\n",
    "    gen_epoch_loss = 0.0\n",
    "    discrim_epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for i, (low_res_batch, high_res_batch) in enumerate(progress_bar):\n",
    "        num_batches += 1\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Push to GPU\n",
    "        low_res_batch = low_res_batch.to(device)\n",
    "        high_res_batch = high_res_batch.to(device)\n",
    "\n",
    "        # Update generator\n",
    "        ############################################################################################\n",
    "        super_resolved = generator(low_res_batch)\n",
    "        natural_probs = discriminator(super_resolved)\n",
    "\n",
    "        pixel_loss = pixel_criterion(super_resolved, high_res_batch)\n",
    "        adversarial_loss = discrim_criterion(\n",
    "            natural_probs, torch.ones_like(natural_probs)\n",
    "        )\n",
    "        gen_loss = pixel_loss\n",
    "\n",
    "        gen_loss += DISCRIM_WEIGHT * adversarial_loss\n",
    "\n",
    "        gen_optimizer.zero_grad()\n",
    "        gen_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "\n",
    "        # Update discriminator\n",
    "        ############################################################################################\n",
    "        true_natural_probs = discriminator(high_res_batch)\n",
    "\n",
    "        # Detach to skip generator computations\n",
    "        fake_natural_probs = discriminator(super_resolved.detach())\n",
    "\n",
    "        true_natural_loss = discrim_criterion(\n",
    "            true_natural_probs, torch.ones_like(true_natural_probs)\n",
    "        )\n",
    "        fake_natural_loss = discrim_criterion(\n",
    "            fake_natural_probs, torch.zeros_like(fake_natural_probs)\n",
    "        )\n",
    "        discrim_loss = true_natural_loss + fake_natural_loss\n",
    "\n",
    "        discrim_optimizer.zero_grad()\n",
    "        discrim_loss.backward()\n",
    "        discrim_optimizer.step()\n",
    "\n",
    "        # Collect data\n",
    "        gen_epoch_loss += gen_loss.item()\n",
    "        discrim_epoch_loss += discrim_loss.item()\n",
    "        progress_bar.set_postfix(\n",
    "            epoch=epoch,\n",
    "            gen_loss=f\"{gen_epoch_loss/num_batches:.8f}\",\n",
    "            discrim_loss=f\"{discrim_epoch_loss/num_batches:.8f}\",\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "        train_time += end_time - start_time\n",
    "\n",
    "    gen_epoch_loss /= len(train_loader)\n",
    "    discrim_epoch_loss /= len(train_loader)\n",
    "    gen_losses.append(gen_epoch_loss)\n",
    "    discrim_losses.append(discrim_epoch_loss)\n",
    "    print(\n",
    "        f\"Epoch: {epoch} / gen_loss: {gen_epoch_loss:.8f} / discrim_loss: {discrim_epoch_loss:.8f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_time / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gen_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_losses[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gen_losses[:75], label=\"Pre-training generator loss\")\n",
    "plt.plot(range(75, len(gen_losses)), gen_losses[75:], label=\"Adversarial learning loss\")\n",
    "# plt.title(\"Generator losses\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([])\n",
    "plt.plot(range(75, len(discrim_losses)), discrim_losses[75:])\n",
    "plt.xlim(left=0)\n",
    "# plt.title(\"Discriminator losses\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Crossentropy Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = {\n",
    "    \"gen_losses\": gen_losses,\n",
    "    \"discrim_losses\": discrim_losses,\n",
    "    \"gen_state\": generator.state_dict(),\n",
    "    \"discrim_state\": discriminator.state_dict(),\n",
    "    \"gen_optimizer_state\": gen_optimizer.state_dict(),\n",
    "    \"discrim_optimizer_state\": discrim_optimizer.state_dict(),\n",
    "    \"gen_learning_rate\": GEN_LEARNING_RATE,\n",
    "    \"discrim_learning_rate\": DISCRIM_LEARNING_RATE,\n",
    "    \"gen_weight_decay\": GEN_WEIGHT_DECAY,\n",
    "    \"discrim_weight_decay\": DISCRIM_WEIGHT_DECAY,\n",
    "    \"discrim_weight\": DISCRIM_WEIGHT,\n",
    "    \"train_time\": train_time,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = RESULTS_DIR / \"continuingbestgen_378epoch.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not save_file.exists():\n",
    "    torch.save(experiment, save_file)\n",
    "    print(f\"Saved to {save_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generated Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BRIGHT_FACTOR = 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(train_patches, batch_size=100)\n",
    "(low_res, high_res) = next(loader.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "index = random.randint(0, len(low_res) - 1)\n",
    "\n",
    "low_res_example = low_res[index]\n",
    "high_res_example = high_res[index]\n",
    "out = generator(low_res_example.unsqueeze(0)).detach().clamp(0, 1)\n",
    "bicubic_out = interpolate(\n",
    "    low_res_example.unsqueeze(0),\n",
    "    size=(256, 256),\n",
    "    mode=\"bicubic\",\n",
    ").float()\n",
    "plot_gallery(\n",
    "    [\n",
    "        adjust_brightness(low_res_example, BRIGHT_FACTOR).permute(1, 2, 0),\n",
    "        adjust_brightness(high_res_example, BRIGHT_FACTOR).permute(1, 2, 0),\n",
    "        adjust_brightness(out[0], BRIGHT_FACTOR).permute(1, 2, 0),\n",
    "        adjust_brightness(bicubic_out[0], BRIGHT_FACTOR).permute(1, 2, 0),\n",
    "    ],\n",
    "    titles=[\"low res\", \"high res\", \"SRGAN\", \"Bicubic\"],\n",
    "    xscale=5,\n",
    "    yscale=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(test_patches, batch_size=300)\n",
    "(low_res, high_res) = next(loader.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "index = random.randint(0, len(low_res) - 1)\n",
    "\n",
    "low_res_example = low_res[index]\n",
    "high_res_example = high_res[index]\n",
    "\n",
    "out = generator(low_res_example.unsqueeze(0)).detach().clamp(0, 1)[0]\n",
    "out = adjust_brightness(out, BRIGHT_FACTOR)\n",
    "\n",
    "bicubic_out = (\n",
    "    interpolate(low_res_example.unsqueeze(0), size=(256, 256), mode=\"bicubic\")\n",
    "    .float()\n",
    "    .clamp(0, 1)\n",
    ")[0]\n",
    "bicubic_out = adjust_brightness(bicubic_out, BRIGHT_FACTOR)\n",
    "\n",
    "low_res_example = adjust_brightness(low_res_example, BRIGHT_FACTOR)\n",
    "high_res_example = adjust_brightness(high_res_example, BRIGHT_FACTOR)\n",
    "\n",
    "plot_gallery(\n",
    "    [\n",
    "        low_res_example.permute(1, 2, 0),\n",
    "        high_res_example.permute(1, 2, 0),\n",
    "        out.permute(1, 2, 0),\n",
    "        bicubic_out.permute(1, 2, 0),\n",
    "    ],\n",
    "    titles=[\"low res\", \"high res\", \"SRGAN\", \"Bicubic\"],\n",
    "    xscale=5,\n",
    "    yscale=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = RESULTS_DIR / \"images/final3_5bright\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srgan_file = IMAGE_DIR / f\"{index}srgan.png\"\n",
    "bicubic_file = IMAGE_DIR / f\"{index}bicubic.png\"\n",
    "low_res_file = IMAGE_DIR / f\"{index}lowres.png\"\n",
    "high_res_file = IMAGE_DIR / f\"{index}highres.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image(low_res_example, low_res_file)\n",
    "save_image(high_res_example, high_res_file)\n",
    "save_image(out, srgan_file)\n",
    "save_image(bicubic_out, bicubic_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from super_resolution.src.testing import compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 256 is largest I can handle on cpu\n",
    "metric_loader = DataLoader(test_patches, batch_size=min(len(test_patches), 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = compute_metrics(lambda x: generator(x).clamp(0, 1), metric_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicubic_metrics = compute_metrics(\n",
    "    lambda x: interpolate(x, size=(256, 256), mode=\"bicubic\"), metric_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicubic_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "getafix_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
