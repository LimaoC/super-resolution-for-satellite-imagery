{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SRCNN notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Increasing the filter size for the second layer has diminishing returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss notes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MSE loss leads to overly blurry images. MSE only takes into account pixel values, and therefore fails to produce output that \"looks good\" to the human eye.\n",
    "- For this reason, we use a perceptual loss function: https://arxiv.org/pdf/1603.08155\n",
    "\n",
    "    - \"We train feed- forward transformation networks for image transformation tasks, but rather than\n",
    "        using per-pixel loss functions depending only on low-level pixel information, we\n",
    "        train our networks using perceptual loss functions that depend on high-level\n",
    "        features from a pretrained loss network. During training, perceptual losses mea-\n",
    "        sure image similarities more robustly than per-pixel losses, and at test-time the\n",
    "        transformation networks run in real-time.\"\n",
    "- Implementation taken from: https://gist.github.com/alper111/8233cdb0414b4cb5853f2f730ab95a49\n",
    "- Perceptual loss seems to noticeably improve the sharpness of the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implemented improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Layer-dependent learning rates.\n",
    "-  Normal initialisation of weights, and zero initialisation for biases.\n",
    "- Implementation of perceptual loss using VGG-16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As in https://arxiv.org/pdf/1603.08155, instead of upsampling using bicubic interpolation, we use a residual neural network to learn how to upsample images.\n",
    "    - \"Downsampling and Upsampling. For super-resolution with an upsampling\n",
    "        factor of f , we use several residual blocks followed by log2 f convolutional layers\n",
    "        with stride 1/2. This is different from [1] who use bicubic interpolation to up-\n",
    "        sample the low-resolution input before passing it to the network. Rather than\n",
    "        relying on a fixed upsampling function, fractionally-strided convolution allows\n",
    "        the upsampling function to be learned jointly with the rest of the network.\"\n",
    "- Train our own VGG-16 network on the satellite imagery. We would need labels to do this. \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
